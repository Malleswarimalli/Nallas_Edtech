# ðŸ§  Cognitive Echo: A Neuroadaptive Learning Prototype  
*Submission for the NALLAS Hackathon*  

**Cognitive Echo** is a functional, open-source prototype that explores the future of personalized education.  
It tackles the challenge of static learning by creating a dynamic experience that adapts to a user's cognitive state in real-time.  

This prototype **simulates a Brain-Computer Interface (BCI)** to understand when a learner is **focused, distracted, or drowsy**, and uses that data to trigger a simple yet effective **visual intervention to enhance memory**.  

---

## âœ¨ Key Innovations & Features  

This prototype demonstrates a **complete, end-to-end neuroadaptive feedback loop**, fulfilling the core requirements of the hackathon challenge.  

### 1. Neuro-Sensing & State Detection (Simulation)  
- One-click simulation of EEG streams (no hardware required).  
- System cycles through **three cognitive states every 3 seconds**, with the UI dynamically adapting to each one.  

- ðŸ”µ **ATTENTION**: User is focused on the learning material. The system *tags* this moment for memory reinforcement.  
- ðŸŸ  **DISTRACTED**: Userâ€™s focus has lapsed.  
- ðŸŸ£ **DROWSY**: User is transitioning towards sleep â€” a key moment for memory consolidation.  

### 2. Adaptive Intervention  
When the **DROWSY** state is detected, the app triggers a subconscious **micro-intervention**:  

- **Visual Cue**: The main card pulses with a gentle glow using **pure CSS animations**.  
- **Non-Intrusive Design**: Subtle, subconscious reinforcement of memory without disturbing the learner.  

---

## ðŸš€ Live Demo  
See **Cognitive Echo** in action as it cycles through its adaptive states.  
UI colors and visual effects change based on simulated brainwave data.  

ðŸ“Œ *(Placeholder for GIF â€” create with GIPHY Capture or ScreenToGif and drag into this README on GitHub)*  
